# Phase 4: Chunking Strategies

[← Back to Implementation Index](./IMPLEMENTATION_INDEX.md)

---

Chunking Strategies

### Learning Objectives
- Understand different chunking approaches
- Learn when to use each strategy
- Design extensible chunking system
- Practice algorithm design

### Design Principles
- **Strategy Pattern**: Different chunking algorithms
- **Configurable Parameters**: Window sizes, gaps as config
- **Extensibility**: Easy to add new strategies

### Implementation Steps

#### Step 4.0: Add Configuration and Dependencies

**Add chunking configuration to `config.py`:**

```python
# Chunking Configuration
CHUNKING_TEMPORAL_WINDOW: int = int(os.getenv("CHUNKING_TEMPORAL_WINDOW", "3600"))  # 1 hour in seconds
CHUNKING_CONVERSATION_GAP: int = int(os.getenv("CHUNKING_CONVERSATION_GAP", "1800"))  # 30 minutes
CHUNKING_WINDOW_SIZE: int = int(os.getenv("CHUNKING_WINDOW_SIZE", "10"))  # messages per window
CHUNKING_OVERLAP: int = int(os.getenv("CHUNKING_OVERLAP", "2"))  # overlapping messages
CHUNKING_MAX_TOKENS: int = int(os.getenv("CHUNKING_MAX_TOKENS", "512"))  # max tokens per chunk
CHUNKING_MIN_CHUNK_SIZE: int = int(os.getenv("CHUNKING_MIN_CHUNK_SIZE", "3"))  # min messages per chunk
```

**Add tiktoken to `requirements.txt`:**

```txt
tiktoken>=0.5.0
```

**Important Note on Message Field Names:**

The database schema uses:
- `message_id` (not `id`)
- `author_name` and `author_display_name` (not just `author`)
- `timestamp` (matches)
- `channel_id` (matches)

All code must use these field names when accessing message dictionaries.

#### Step 4.1: Design Chunk Data Structure

Create `chunking/base.py` (Chunk data structure) and `chunking/service.py` (ChunkingService):

**Note**: In the refactored architecture, chunking strategies can be split into `chunking/strategies/` files (temporal.py, conversation.py, token_aware.py, sliding_window.py), but for learning purposes, we'll show them together here.

```python
from typing import List, Dict, Optional
from datetime import datetime
from config import Config

class Chunk:
    """
    Data structure for a chunk.
    
    Learning: Good data modeling is crucial for RAG systems.
    Metadata enables filtering and context preservation.
    """
    def __init__(
        self,
        content: str,
        message_ids: List[str],
        metadata: Dict
    ):
        self.content = content
        self.message_ids = message_ids
        self.metadata = metadata
    
    def to_dict(self) -> Dict:
        """Convert chunk to dictionary for storage"""
        return {
            "content": self.content,
            "message_ids": self.message_ids,
            "metadata": self.metadata
        }

class ChunkingService:
    """
    Service for chunking messages using different strategies.
    
    Learning: Strategy pattern allows experimenting with different approaches.
    """
    
    def __init__(
        self,
        temporal_window: int = None,
        conversation_gap: int = None
    ):
        import logging
        self.logger = logging.getLogger(__name__)
        self.temporal_window = temporal_window or Config.CHUNKING_TEMPORAL_WINDOW
        self.conversation_gap = conversation_gap or Config.CHUNKING_CONVERSATION_GAP
    
    def _validate_messages(self, messages: List[Dict]) -> bool:
        """
        Validate that messages list is valid for chunking.
        
        Returns:
            True if valid, raises ValueError if invalid
        """
        if not isinstance(messages, list):
            raise ValueError(f"Messages must be a list, got {type(messages)}")
        if not messages:
            return True  # Empty list is valid (will return empty chunks)
        # Check that first message has required fields
        sample = messages[0]
        if not isinstance(sample, dict):
            raise ValueError(f"Messages must be dictionaries, got {type(sample)}")
        return True
```

#### Step 4.2: Implement Temporal Chunking

```python
def chunk_temporal(self, messages: List[Dict]) -> List[Chunk]:
    """
    Group messages by time windows.
    
    Learning: Temporal chunking preserves time-based context.
    Useful for conversations that happen over time.
    """
    self._validate_messages(messages)
    if not messages:
        return []
    
    # Sort messages by timestamp
    sorted_messages = sorted(
        messages, 
        key=lambda m: m.get('timestamp', '')
    )
    
    chunks = []
    current_chunk = []
    window_start = None
    
    for message in sorted_messages:
        try:
            # Parse timestamp
            timestamp_str = message.get('timestamp', '')
            if 'Z' in timestamp_str:
                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            else:
                timestamp = datetime.fromisoformat(timestamp_str)
        except Exception:
            # Skip messages with invalid timestamps
            continue
        
        # Start new window if needed
        if window_start is None:
            window_start = timestamp
        
        # Check if message is outside current window
        time_diff = (timestamp - window_start).total_seconds()
        
        if time_diff > self.temporal_window:
            # Save current chunk and start new one
            if current_chunk:
                chunks.append(self._create_chunk(current_chunk, "temporal"))
            current_chunk = [message]
            window_start = timestamp
        else:
            current_chunk.append(message)
    
    # Don't forget last chunk
    if current_chunk:
        chunks.append(self._create_chunk(current_chunk, "temporal"))
    
    return chunks
```

#### Step 4.3: Implement Conversation Chunking

```python
def chunk_conversation(self, messages: List[Dict]) -> List[Chunk]:
    """
    Group messages by conversation boundaries.
    
    Learning: Conversation chunking detects natural breaks in dialogue.
    Boundaries: time gaps, channel changes, topic shifts.
    """
    self._validate_messages(messages)
    if not messages:
        return []
    
    sorted_messages = sorted(
        messages,
        key=lambda m: m.get('timestamp', '')
    )
    
    chunks = []
    current_chunk = []
    last_timestamp = None
    
    for message in sorted_messages:
        try:
            timestamp_str = message.get('timestamp', '')
            if 'Z' in timestamp_str:
                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            else:
                timestamp = datetime.fromisoformat(timestamp_str)
        except Exception:
            continue
        
        channel_id = message.get('channel_id')
        
        # Check for conversation boundary
        is_boundary = False
        
        if last_timestamp:
            time_gap = (timestamp - last_timestamp).total_seconds()
            if time_gap > self.conversation_gap:
                is_boundary = True
        
        # Channel change is also a boundary
        if current_chunk and current_chunk[-1].get('channel_id') != channel_id:
            is_boundary = True
        
        if is_boundary and current_chunk:
            chunks.append(self._create_chunk(current_chunk, "conversation"))
            current_chunk = [message]
        else:
            current_chunk.append(message)
        
        last_timestamp = timestamp
    
    if current_chunk:
        chunks.append(self._create_chunk(current_chunk, "conversation"))
    
    return chunks
```

#### Step 4.4: Implement Single Message Chunking

```python
def chunk_single(self, messages: List[Dict]) -> List[Chunk]:
    """
    Each message is its own chunk.
    
    Learning: Baseline strategy for comparison.
    Maximum granularity, preserves all individual messages.
    """
    self._validate_messages(messages)
    chunks = []
    for message in messages:
        chunks.append(self._create_chunk([message], "single"))
    return chunks

def _create_chunk(self, messages: List[Dict], strategy: str) -> Chunk:
    """Helper to create chunk with metadata"""
    if not messages:
        raise ValueError("Cannot create chunk from empty messages")
    
    # Format content
    content_parts = []
    for msg in messages:
        # Use author_display_name if available, fallback to author_name, then 'Unknown'
        author = msg.get('author_display_name') or msg.get('author_name') or 'Unknown'
        timestamp = msg.get('timestamp', '')[:10]  # Date only
        content = msg.get('content', '').strip()
        if content:
            content_parts.append(f"{timestamp} - {author}: {content}")
    
    content = "\n".join(content_parts)
    
    # Collect metadata - use message_id (database field name)
    message_ids = [str(msg.get('message_id', '')) for msg in messages if msg.get('message_id')]
    metadata = {
        "chunk_strategy": strategy,
        "channel_id": messages[0].get('channel_id', ''),
        "message_count": len(messages),
        "first_message_id": message_ids[0] if message_ids else '',
        "last_message_id": message_ids[-1] if message_ids else '',
        "first_timestamp": messages[0].get('timestamp', ''),
        "last_timestamp": messages[-1].get('timestamp', ''),
    }
    
    return Chunk(content, message_ids, metadata)
```

#### Step 4.5: Add Token Counting Utility

```python
import tiktoken

def count_tokens(self, text: str, model: str = "cl100k_base") -> int:
    """
    Count tokens in text using tiktoken.

    Learning: Different models have different tokenizers.
    - cl100k_base: GPT-4, GPT-3.5-turbo
    - p50k_base: GPT-3 (davinci, curie, etc.)

    Why this matters:
    - Embedding models have max token limits (384-8192 tokens)
    - LLMs have context limits (4k-128k tokens)
    - Discord messages can be 2000 chars = ~500 tokens
    """
    try:
        encoding = tiktoken.get_encoding(model)
        return len(encoding.encode(text))
    except Exception:
        # Fallback: rough estimate (4 chars per token)
        return len(text) // 4
```

#### Step 4.6: Implement Sliding Window Chunking

```python
def chunk_sliding_window(
    self,
    messages: List[Dict],
    window_size: int = 10,
    overlap: int = 2
) -> List[Chunk]:
    """
    Create overlapping chunks with sliding window.

    Learning: Overlap prevents losing context at chunk boundaries.
    Critical for RAG to find information that spans multiple messages.

    Example (window=3, overlap=1):
        Chunk 1: [msg1, msg2, msg3]
        Chunk 2: [msg3, msg4, msg5]  # msg3 overlaps
        Chunk 3: [msg5, msg6, msg7]  # msg5 overlaps

    Why overlap matters:
    - Conversations often span chunk boundaries
    - Overlap ensures queries can match across boundaries
    - Slight redundancy improves retrieval quality

    Args:
        messages: List of messages to chunk
        window_size: Number of messages per chunk
        overlap: Number of messages to overlap between chunks

    Returns:
        List of chunks with overlapping messages
    """
    self._validate_messages(messages)
    if not messages:
        return []

    if overlap >= window_size:
        self.logger.warning(
            f"Overlap ({overlap}) >= window_size ({window_size}), "
            f"setting overlap to {window_size - 1}"
        )
        overlap = max(1, window_size - 1)

    sorted_messages = sorted(messages, key=lambda m: m.get('timestamp', ''))
    chunks = []

    start = 0
    while start < len(sorted_messages):
        end = min(start + window_size, len(sorted_messages))
        window_messages = sorted_messages[start:end]

        if window_messages:
            chunks.append(self._create_chunk(window_messages, "sliding_window"))

        # Move forward by (window_size - overlap)
        start += window_size - overlap

        # Prevent infinite loop if we're at the end
        if end == len(sorted_messages):
            break

    return chunks
```

#### Step 4.7: Implement Token-Aware Chunking

```python
def chunk_by_tokens(
    self,
    messages: List[Dict],
    max_tokens: int = 512,
    min_chunk_size: int = 3
) -> List[Chunk]:
    """
    Create chunks respecting token limits.

    Learning: Embedding models have max input sizes:
    - all-MiniLM-L6-v2: 512 tokens
    - all-mpnet-base-v2: 512 tokens
    - OpenAI ada-002: 8192 tokens
    - text-embedding-3-small: 8192 tokens

    Why this matters:
    - Exceeding token limits causes errors or truncation
    - Different strategies produce different chunk sizes
    - Need to validate chunks before embedding

    Args:
        messages: List of messages to chunk
        max_tokens: Maximum tokens per chunk
        min_chunk_size: Minimum messages per chunk (avoid tiny chunks)

    Returns:
        List of chunks within token limits
    """
    self._validate_messages(messages)
    if not messages:
        return []

    sorted_messages = sorted(messages, key=lambda m: m.get('timestamp', ''))
    chunks = []
    current_chunk = []
    current_tokens = 0

    for message in sorted_messages:
        # Format message to see actual token count
        # Use author_display_name if available, fallback to author_name
        author = message.get('author_display_name') or message.get('author_name') or 'Unknown'
        timestamp = message.get('timestamp', '')[:10]
        content = message.get('content', '').strip()

        if not content:
            continue

        formatted = f"{timestamp} - {author}: {content}\n"
        msg_tokens = self.count_tokens(formatted)

        # If single message exceeds limit, split it
        if msg_tokens > max_tokens:
            # Save current chunk if it exists
            if current_chunk and len(current_chunk) >= min_chunk_size:
                chunks.append(self._create_chunk(current_chunk, "token_aware"))
                current_chunk = []
                current_tokens = 0

            # Handle oversized message (truncate or skip)
            self.logger.warning(
                f"Message {message.get('message_id', 'unknown')} exceeds {max_tokens} tokens "
                f"({msg_tokens} tokens). Skipping."
            )
            continue

        # Check if adding this message would exceed limit
        if current_tokens + msg_tokens > max_tokens:
            # Save current chunk if it meets minimum size
            if current_chunk and len(current_chunk) >= min_chunk_size:
                chunks.append(self._create_chunk(current_chunk, "token_aware"))
            elif current_chunk:
                self.logger.warning(
                    f"Chunk too small ({len(current_chunk)} messages), "
                    f"but at token limit. Creating anyway."
                )
                chunks.append(self._create_chunk(current_chunk, "token_aware"))

            # Start new chunk
            current_chunk = [message]
            current_tokens = msg_tokens
        else:
            # Add message to current chunk
            current_chunk.append(message)
            current_tokens += msg_tokens

    # Don't forget last chunk
    if current_chunk and len(current_chunk) >= min_chunk_size:
        chunks.append(self._create_chunk(current_chunk, "token_aware"))
    elif current_chunk:
        self.logger.warning(
            f"Last chunk too small ({len(current_chunk)} messages). "
            f"Creating anyway to avoid data loss."
        )
        chunks.append(self._create_chunk(current_chunk, "token_aware"))

    return chunks
```

#### Step 4.8: Update _create_chunk with Validation

```python
def _create_chunk(self, messages: List[Dict], strategy: str) -> Chunk:
    """Helper to create chunk with metadata and validation"""
    if not messages:
        raise ValueError("Cannot create chunk from empty messages")

    # Format content
    content_parts = []
    for msg in messages:
        # Use author_display_name if available, fallback to author_name, then 'Unknown'
        author = msg.get('author_display_name') or msg.get('author_name') or 'Unknown'
        timestamp = msg.get('timestamp', '')[:10]  # Date only
        content = msg.get('content', '').strip()
        if content:
            content_parts.append(f"{timestamp} - {author}: {content}")

    content = "\n".join(content_parts)

    # Validate token count
    token_count = self.count_tokens(content)

    # Collect metadata - use message_id (database field name)
    message_ids = [str(msg.get('message_id', '')) for msg in messages if msg.get('message_id')]
    metadata = {
        "chunk_strategy": strategy,
        "channel_id": messages[0].get('channel_id', ''),
        "message_count": len(messages),
        "token_count": token_count,  # ⭐ NEW: Track token count
        "first_message_id": message_ids[0] if message_ids else '',
        "last_message_id": message_ids[-1] if message_ids else '',
        "first_timestamp": messages[0].get('timestamp', ''),
        "last_timestamp": messages[-1].get('timestamp', ''),
    }

    # Log warnings for problematic chunks
    if token_count > 512:
        self.logger.warning(
            f"Chunk exceeds 512 tokens ({token_count}). "
            f"May fail with some embedding models."
        )

    if len(messages) == 1:
        self.logger.debug(f"Single-message chunk created (strategy: {strategy})")

    return Chunk(content, message_ids, metadata)
```

#### Step 4.9: Main Chunking Method

```python
def chunk_messages(
    self,
    messages: List[Dict],
    strategies: List[str] = None
) -> Dict[str, List[Chunk]]:
    """
    Generate chunks using specified strategies.

    Learning: Returns all strategies at once for comparison.

    Available strategies:
    - temporal: Time-based windows
    - conversation: Gap detection
    - single: One message per chunk
    - sliding_window: Overlapping windows (NEW)
    - token_aware: Respect token limits (NEW)
    """
    if strategies is None:
        strategies = ["temporal", "conversation", "sliding_window", "token_aware"]

    results = {}

    if "temporal" in strategies:
        results["temporal"] = self.chunk_temporal(messages)

    if "conversation" in strategies:
        results["conversation"] = self.chunk_conversation(messages)

    if "single" in strategies:
        results["single"] = self.chunk_single(messages)

    if "sliding_window" in strategies:
        results["sliding_window"] = self.chunk_sliding_window(
            messages,
            window_size=Config.CHUNKING_WINDOW_SIZE,
            overlap=Config.CHUNKING_OVERLAP
        )

    if "token_aware" in strategies:
        results["token_aware"] = self.chunk_by_tokens(
            messages,
            max_tokens=Config.CHUNKING_MAX_TOKENS,
            min_chunk_size=Config.CHUNKING_MIN_CHUNK_SIZE
        )

    return results
```

#### Step 4.10: Message Retrieval Integration

**Note on Message Format:**

When retrieving messages from `MessageStorage`, you'll get dictionaries with these fields:
- `message_id` (primary key)
- `channel_id`
- `content`
- `author_name` and `author_display_name`
- `timestamp` (ISO format string)
- Other fields as defined in the schema

**Example usage with MessageStorage:**

```python
from storage.messages.messages import MessageStorage
from chunking.service import ChunkingService

# Retrieve messages from database
storage = MessageStorage()
messages = storage.get_channel_messages(channel_id="123456789")

# Convert to format expected by chunking service
# (MessageStorage already returns dicts with correct field names)
chunking_service = ChunkingService()
chunks = chunking_service.chunk_messages(messages, strategies=["token_aware", "sliding_window"])
```

**Optional: Add helper method to ChunkingService:**

```python
def chunk_from_storage(
    self,
    storage: MessageStorage,
    channel_id: str,
    strategies: List[str] = None
) -> Dict[str, List[Chunk]]:
    """
    Convenience method to chunk messages directly from storage.
    
    Args:
        storage: MessageStorage instance
        channel_id: Channel ID to retrieve messages from
        strategies: List of chunking strategies to use
        
    Returns:
        Dictionary mapping strategy names to lists of chunks
    """
    messages = storage.get_channel_messages(channel_id)
    return self.chunk_messages(messages, strategies=strategies)
```

#### Step 4.11: Add Chunk Validation Method

```python
def validate_chunk(self, chunk: Chunk, max_tokens: int = 512) -> bool:
    """
    Validate that a chunk is within token limits.
    
    Args:
        chunk: Chunk to validate
        max_tokens: Maximum allowed tokens
        
    Returns:
        True if valid, False otherwise
    """
    token_count = chunk.metadata.get('token_count', 0)
    if token_count == 0:
        # Recalculate if not in metadata
        token_count = self.count_tokens(chunk.content)
    
    if token_count > max_tokens:
        self.logger.warning(
            f"Chunk {chunk.metadata.get('first_message_id', 'unknown')} "
            f"exceeds {max_tokens} tokens ({token_count} tokens)"
        )
        return False
    
    return True

def get_chunk_statistics(self, chunks: List[Chunk]) -> Dict:
    """
    Get statistics about a list of chunks.
    
    Useful for debugging and optimization.
    
    Returns:
        Dictionary with statistics
    """
    if not chunks:
        return {
            "total_chunks": 0,
            "total_messages": 0,
            "avg_tokens_per_chunk": 0,
            "max_tokens": 0,
            "min_tokens": 0,
        }
    
    token_counts = [c.metadata.get('token_count', 0) for c in chunks]
    message_counts = [c.metadata.get('message_count', 0) for c in chunks]
    
    return {
        "total_chunks": len(chunks),
        "total_messages": sum(message_counts),
        "avg_tokens_per_chunk": sum(token_counts) / len(chunks) if token_counts else 0,
        "max_tokens": max(token_counts) if token_counts else 0,
        "min_tokens": min(token_counts) if token_counts else 0,
        "avg_messages_per_chunk": sum(message_counts) / len(chunks) if message_counts else 0,
    }
```

#### Step 4.12: Update chunking/__init__.py

```python
"""
Chunking module for RAG system.

Provides chunking strategies for Discord messages.
"""

from chunking.base import Chunk
from chunking.service import ChunkingService

__all__ = ["Chunk", "ChunkingService"]
```

### Common Pitfalls - Phase 4

1. **Field name mismatches**: Use `message_id` (not `id`) and `author_name`/`author_display_name` (not `author`) to match database schema
2. **Missing logger**: Always initialize `self.logger = logging.getLogger(__name__)` in `__init__`
3. **Missing config**: Add all `CHUNKING_*` config attributes to `config.py` before using them
4. **Missing dependency**: Add `tiktoken>=0.5.0` to `requirements.txt`
5. **Timestamp parsing**: Handle both ISO formats (with/without Z)
6. **Empty chunks**: Don't create chunks with no messages
7. **Input validation**: Always validate messages list before processing (use `_validate_messages`)
8. **Sorting**: Always sort by timestamp before chunking
9. **Boundary detection**: Time gaps must account for timezone
10. **Metadata**: Include all info needed for filtering later
11. **Token counting**: Don't forget to install tiktoken (`pip install tiktoken`)
12. **Overlap validation**: Overlap must be less than window_size
13. **Token limits**: Different embedding models have different max tokens
14. **Tiny chunks**: Set minimum chunk size to avoid single-message chunks (unless intended)
15. **Message retrieval**: Ensure messages from `MessageStorage` use correct field names before chunking

### Debugging Tips - Phase 4

- **Use chunk statistics**: Call `get_chunk_statistics()` to analyze chunk distribution
- **Validate chunks**: Use `validate_chunk()` before embedding to catch token limit issues
- **Print chunk sizes**: See how many messages per chunk
- **Check timestamps**: Verify they're parsed correctly
- **Test boundaries**: Create test data with known gaps
- **Compare strategies**: Visualize chunk boundaries
- **Verify token counts**: Log token counts for each chunk (now in metadata)
- **Test overlap**: Verify messages appear in multiple chunks with sliding_window
- **Monitor warnings**: Watch for oversized messages or small chunks
- **Check field names**: Verify messages use `message_id` and `author_name`/`author_display_name`
- **Validate inputs**: Use `_validate_messages()` to catch type errors early

### Performance Considerations - Phase 4

- **Sorting**: O(n log n) complexity, but necessary
- **Chunk count**: More chunks = more embeddings = more storage
- **Content length**: Keep chunks under token limits
- **Token counting**: tiktoken is fast but still adds overhead
- **Strategy selection**:
  - sliding_window: More chunks (overlap), better retrieval
  - token_aware: Fewer, optimally-sized chunks
  - temporal/conversation: Variable size, depends on data

### Recommended Strategy Combinations

For most use cases:
1. **Start with**: `token_aware` (respects limits, good baseline)
2. **Add**: `sliding_window` (better retrieval with overlap)
3. **Compare**: Both strategies to see which performs better
4. **Optional**: `conversation` if natural breaks are important

For experimentation:
- Try all strategies and compare retrieval quality (Phase 6.5)