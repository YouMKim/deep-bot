# Discord RAG Chatbot - Complete Implementation Guide ðŸ¤–

Build a production-ready Discord bot with RAG (Retrieval-Augmented Generation) that can answer questions about your server's chat history using AI and semantic search.

**What you'll build:**
- Discord bot that remembers all conversations
- Semantic search through thousands of messages
- AI-powered question answering
- Multiple RAG strategies (basic to advanced)
- Secure, production-ready deployment

**Time commitment:**
- Quick demo: 5 minutes ([QUICKSTART.md](QUICKSTART.md))
- Basic chatbot: 3-4 hours (Phases 0-2)
- Full RAG system: 20-30 hours (Phases 0-10)
- Production-ready: 40-60 hours (all phases)

**Cost:**
- $0/month with local AI (Ollama + sentence-transformers)
- $5-20/month with OpenAI for better quality/speed

---

## ðŸ“š Getting Started

### New to this project?

1. **Try the 5-minute demo** â†’ [QUICKSTART.md](QUICKSTART.md)
   - Get a basic bot running immediately
   - See what you're building
   - No coding required for demo

2. **Read the FAQ** â†’ [FAQ.md](FAQ.md)
   - Common questions answered
   - Troubleshooting help
   - Understand key concepts

3. **Plan your budget** â†’ [COST_MANAGEMENT.md](COST_MANAGEMENT.md)
   - Understand costs (free to $20/month)
   - Budget calculator
   - Cost optimization tips

4. **Start learning** â†’ Begin with [Phase 0](#phase-0-development-environment-setup)
   - Set up your development environment
   - Install dependencies
   - Create your Discord bot

### Already familiar with Discord bots?

- **Quick path:** Jump to [Phase 2](#phase-2-simple-mvp-chatbot-quick-win) for immediate AI chatbot
- **Full path:** Start at [Phase 0](#phase-0-development-environment-setup) for complete setup

---

## ðŸ“– Table of Contents

### ðŸš€ Quick Start & Foundation
- **[QUICKSTART.md](QUICKSTART.md)** - 5-minute demo bot
- **[FAQ.md](FAQ.md)** - Frequently asked questions
- **[COST_MANAGEMENT.md](COST_MANAGEMENT.md)** - Budget planning & cost tracking
- **[Phase 0: Development Environment Setup](./PHASE_00.md)** ðŸ†• NEW â­ START HERE
- **[Phase 2: Simple MVP Chatbot](./PHASE_02_MVP.md)** ðŸ†• NEW - Quick win! (Note: numbered Phase 2 to keep existing phases)
- **[Phase 3: Security Fundamentals](./PHASE_03_SECURITY.md)** ðŸ†• NEW - Essential security early

### ðŸ“¦ Core RAG Implementation
- **Phase 1:** [Foundation - Message Storage Abstraction](./PHASE_01.md)
- **Phase 2 (old):** [Rate Limiting & API Design](./PHASE_02.md) â­ UPDATED
- **Phase 3 (old):** [Embedding Service Abstraction](./PHASE_03.md)
- **Phase 4:** [Chunking Strategies](./PHASE_04.md) â­ UPDATED - Token limits & sliding window
- **Phase 5:** [Vector Store Abstraction](./PHASE_05.md)
- **Phase 6:** [Multi-Strategy Chunk Storage](./PHASE_06.md)
- **Phase 6.5:** [Strategy Evaluation & Comparison](./PHASE_06_5.md) ðŸ†• NEW
- **Phase 7:** [Bot Commands Integration](./PHASE_07.md)
- **Phase 8:** [Summary Enhancement](./PHASE_08.md)
- **Phase 9:** [Configuration & Polish](./PHASE_09.md) â­ UPDATED - RAG config
- **Phase 10:** [RAG Query Pipeline](./PHASE_10.md) ðŸ†• NEW

### ðŸ§  Advanced Chatbot Features
- **Phase 11:** [Conversational Chatbot with Memory](./PHASE_11.md) ðŸ†• NEW
- **Phase 12:** [User Emulation Mode](./PHASE_12.md) ðŸ†• NEW
- **Phase 13:** [Debate & Rhetoric Analyzer](./PHASE_13.md) ðŸ†• NEW

### ðŸ”¥ Advanced RAG Techniques
- **Phase 14:** [Hybrid Search (Vector + Keyword)](./PHASE_14.md) ðŸ”¥ NEW
- **Phase 15:** [Reranking & Query Optimization](./PHASE_15.md) ðŸ”¥ NEW
- **Phase 16:** [Advanced RAG Techniques (HyDE, Self-RAG, RAG Fusion)](./PHASE_16.md) ðŸ”¥ NEW
- **Phase 17:** [RAG Strategy Comparison Dashboard](./PHASE_17.md) ðŸ”¥ NEW

### ðŸ”’ Security & Production
- **Phase 18:** [Security & Prompt Injection Defense](./PHASE_18.md) ðŸ”’ NEW
- **[CODE_REVIEW.md](./CODE_REVIEW.md)** âš ï¸ NEW - Review of existing code with 20 improvements

### ðŸš¢ Deployment & Operations
- **[DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md)** ðŸ†• NEW
- [Quick Reference Checklists](#quick-reference)
- [Common Pitfalls & Debugging](#common-pitfalls)

### ðŸ“Š Project Planning & Assessment
- **[PROJECT_ASSESSMENT.md](./PROJECT_ASSESSMENT.md)** ðŸ“‹ NEW - What's missing & future phases
- **[PROJECT_REVIEW_AND_IMPROVEMENTS.md](./PROJECT_REVIEW_AND_IMPROVEMENTS.md)** âš ï¸ NEW - Critical review

---

## ðŸŽ¯ Recommended Learning Paths

Choose your path based on your goals and experience:

### Path 1: Quick Demo (5 minutes) ðŸƒâ€â™‚ï¸

**Goal:** See a working bot immediately

1. [QUICKSTART.md](QUICKSTART.md) - Clone, configure, run!

**Result:** Basic bot that responds to commands (no AI yet)

---

### Path 2: Simple AI Chatbot (3-4 hours) âš¡

**Goal:** Working AI chatbot that answers questions about recent messages

1. [Phase 0: Development Environment Setup](./PHASE_00.md) - 30-60 min
   - Python, Discord bot, environment variables
   - Optional: Ollama (free local AI)

2. [Phase 2: Simple MVP Chatbot](./PHASE_02_MVP.md) - 1 hour
   - Basic AI chatbot using last 50 messages
   - Works with OpenAI or free Ollama
   - Instant gratification!

3. [Phase 3: Security Fundamentals](./PHASE_03_SECURITY.md) - 2 hours
   - Input validation, rate limiting
   - Basic prompt injection defense
   - Error handling

**Result:** Working AI chatbot that answers questions about recent chat history

**Cost:** $0 (Ollama) or ~$0.50/month (OpenAI with light usage)

---

### Path 3: Full RAG System (20-30 hours) ðŸš€

**Goal:** Production-quality RAG chatbot with semantic search

**Complete Paths 1-2 first**, then continue:

4. **Phase 1:** Message Storage (2-3 hours)
   - SQLite database for messages
   - Checkpoint/resume system
   - Query optimization

5. **Phase 4:** Token-Aware Chunking (2-3 hours)
   - Intelligent message chunking
   - Multiple strategies (temporal, conversational)
   - Token limit handling

6. **Phase 5:** Embedding Service (2-3 hours)
   - Convert text to vectors
   - Local (free) or cloud (paid) embeddings
   - Batch processing

7. **Phase 6:** Vector Database (2-3 hours)
   - ChromaDB for semantic search
   - Store message embeddings
   - Query by similarity

8. **Phase 7-10:** RAG Pipeline (6-8 hours)
   - Complete RAG query system
   - Bot command integration
   - Strategy evaluation
   - Configuration & polish

**Result:** Full RAG chatbot that can search through thousands of messages semantically

**Cost:** $0-5/month (depending on AI provider choice)

---

### Path 4: Advanced Features (40-60 hours) ðŸŽ“

**Goal:** Master advanced RAG techniques and production deployment

**Complete Path 3 first**, then choose from:

**Advanced RAG (Phases 14-17):** 10-15 hours
- Hybrid search (vector + keyword)
- Cross-encoder reranking
- HyDE, Self-RAG, RAG Fusion
- Strategy comparison dashboard

**Advanced Chatbot (Phases 11-13):** 8-10 hours
- Multi-turn conversations with memory
- User personality emulation
- Debate & rhetoric analysis

**Advanced Security (Phase 18):** 4-6 hours
- Multi-layer defense (8 layers)
- ML-based prompt injection detection
- Security audit logging

**Production Deployment:** 4-6 hours
- Docker containerization
- Cloud deployment (Railway, AWS, etc.)
- Monitoring & logging
- Backup strategies

**Result:** Production-ready, feature-complete RAG chatbot system

**Cost:** $10-50/month (depending on scale and providers)

---

## ðŸ† Milestone-Based Learning Path

**For building a complete RAG system:**

> **Note on Phase Numbering:** For backwards compatibility, new foundational phases (Phase 0, new Phase 2, new Phase 3) use available numbers, while existing phase files retain their original numbers (old Phase 2, old Phase 3, etc.). Follow the **Recommended Learning Paths** above for the correct order.

### Milestone 0: Foundation Setup âœ… NEW
- âœ… **Phase 0:** Development environment setup
- âœ… **Phase 2 (new):** Simple MVP chatbot (quick win!)
- âœ… **Phase 3 (new):** Security fundamentals
- **Goal:** Working AI chatbot with basic security in 3-4 hours
- **Why first:** Provides immediate gratification and establishes security habits early

### Milestone 1: Data Collection
- âœ… Phase 1: SQLite storage with checkpoints
- âœ… Phase 2: Rate-limited Discord API loading
- **Goal:** Store 10,000+ messages from your Discord server

### Milestone 2: Embedding & Chunking (Phases 3-4)
- âœ… Phase 3: Embedding provider abstraction (local + cloud)
- âœ… Phase 4: Multiple chunking strategies (temporal, sliding window, token-aware)
- **Goal:** Create multiple chunking approaches for comparison

### Milestone 3: Vector Storage & Evaluation (Phases 5-6.5)
- âœ… Phase 5: Vector store abstraction (ChromaDB)
- âœ… Phase 6: Multi-strategy chunk storage
- ðŸ†• Phase 6.5: Evaluate and compare strategies
- **Goal:** Identify the best chunking strategy for your data

### Milestone 4: RAG Query Pipeline (Phases 7-10)
- âœ… Phase 7: Bot commands integration
- âœ… Phase 8: Summary enhancement
- âœ… Phase 9: Configuration management
- ðŸ†• Phase 10: Complete RAG query pipeline
- **Goal:** Working end-to-end RAG chatbot

### Milestone 5: Advanced Chatbot Features (Phases 11-13) ðŸ†•
- ðŸ†• Phase 11: Conversational chatbot with memory
  - Multi-turn conversations
  - Context-aware responses
  - Combines RAG + conversation history
- ðŸ†• Phase 12: User emulation mode
  - Mimic user speech patterns
  - Personality modeling
  - Style transfer
- ðŸ†• Phase 13: Debate & rhetoric analyzer
  - Argument structure analysis
  - Logical fallacy detection
  - Fact-checking with RAG
  - Constructive feedback generation
- **Goal:** Fully-featured conversational AI chatbot

### Milestone 6: Production Deployment ðŸš€
- Deploy with Docker & Docker Compose
- Choose deployment platform (Railway, VPS, AWS, etc.)
- Set up monitoring & logging
- Implement backups
- Configure security
- **Goal:** Production-ready, scalable system

See [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md) for complete deployment instructions.

### Milestone 7: Advanced RAG Techniques ðŸ”¥ NEW
- ðŸ”¥ Phase 14: Hybrid Search (Vector + Keyword)
  - BM25 keyword search
  - Reciprocal Rank Fusion (RRF)
  - Compare vector vs keyword vs hybrid
- ðŸ”¥ Phase 15: Reranking & Query Optimization
  - Cross-encoder reranking
  - Query expansion techniques
  - Performance vs accuracy trade-offs
- ðŸ”¥ Phase 16: Advanced RAG Methods
  - HyDE (Hypothetical Document Embeddings)
  - Self-RAG (Self-Reflective Retrieval)
  - RAG Fusion (Multi-Query Synthesis)
  - Iterative retrieval
- ðŸ”¥ Phase 17: RAG Strategy Comparison Dashboard
  - Unified RAG service
  - Strategy recommendation engine
  - A/B testing framework
  - Interactive comparison tools
- **Goal:** Master state-of-the-art RAG techniques and understand when to use each

### Milestone 8: Security & Production Hardening ðŸ”’ NEW
- ðŸ”’ Phase 18: Security & Prompt Injection Defense
  - Prompt injection detection & prevention
  - Multi-layer defense strategy
  - Input sanitization & validation
  - System prompt protection
  - Output validation
  - Rate limiting per user
  - Security audit logging
- **Goal:** Production-ready, secure RAG system resistant to attacks

---

## ðŸ—ï¸ Project Architecture

### Folder Structure

This project uses a **domain-based architecture** for better organization and maintainability:

```
deep-bot/
â”œâ”€â”€ embedding/              # Text â†’ vector embeddings
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ sentence_transformer.py
â”‚   â”œâ”€â”€ openai.py
â”‚   â””â”€â”€ factory.py
â”‚
â”œâ”€â”€ chunking/              # Message chunking strategies
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ service.py
â”‚   â””â”€â”€ strategies/
â”‚
â”œâ”€â”€ retrieval/             # Vector storage & search
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ factory.py
â”‚   â””â”€â”€ providers/
â”‚       â””â”€â”€ chroma.py
â”‚
â”œâ”€â”€ storage/               # Data persistence
â”‚   â””â”€â”€ message_storage.py
â”‚
â”œâ”€â”€ rag/                   # RAG orchestration
â”‚   â”œâ”€â”€ memory_service.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ ai/                    # LLM abstraction
â”‚   â””â”€â”€ service.py
â”‚
â”œâ”€â”€ security/              # Security layer
â”‚   â”œâ”€â”€ input_validator.py
â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â””â”€â”€ prompt_injection.py
â”‚
â”œâ”€â”€ bot/                   # Discord bot
â”‚   â”œâ”€â”€ cogs/
â”‚   â”œâ”€â”€ loaders/
â”‚   â””â”€â”€ utils/
â”‚
â””â”€â”€ utils/                 # General utilities
    â””â”€â”€ ...
```

**Why this structure?**
- âœ… Clear separation of concerns (RAG vs Bot vs Security)
- âœ… Easy to navigate ("Where's embedding code?" â†’ `embedding/`)
- âœ… Scales well (add new provider â†’ `retrieval/providers/new.py`)
- âœ… Testing boundaries explicit
- âœ… Team-friendly for collaboration

**Import style:**
```python
# Clean, domain-based imports
from embedding import EmbeddingFactory
from chunking import ChunkingService
from retrieval import VectorStoreFactory
from storage import MessageStorage
from rag import ChunkedMemoryService
```

**Need to refactor?** See [REFACTORING_PLAN.md](./REFACTORING_PLAN.md) for migration guide.

---

## Phase 1: Foundation - Message Storage Abstraction

### Learning Objectives
- Understand when to use SQLite vs JSON files
- Learn checkpoint/resume patterns for long-running operations
- Practice database schema design
- Learn transaction management
- Understand connection pooling and context managers

### Design Principles
- **Separation of Concerns**: Storage logic separated from business logic
- **Database Abstraction**: Design interface that could swap databases later
- **Transaction Management**: Ensure data integrity with atomic operations
- **Resource Management**: Use context managers for proper cleanup

### Implementation Steps

#### Step 1.1: Design Database Schema

Create `storage/message_storage.py` and design the schema:

```python
# Schema design considerations:
# - Primary keys for fast lookups
# - Indexes on frequently queried columns (channel_id, timestamp)
# - JSON blob for flexible metadata storage
# - Timestamps for debugging and auditing

SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS messages (
    message_id TEXT PRIMARY KEY,
    channel_id TEXT NOT NULL,
    guild_id TEXT,
    content TEXT NOT NULL,
    author_id TEXT NOT NULL,
    author_name TEXT,
    author_display_name TEXT,
    channel_name TEXT,
    guild_name TEXT,
    timestamp TEXT NOT NULL,
    created_at TEXT NOT NULL,
    is_bot INTEGER DEFAULT 0,
    has_attachments INTEGER DEFAULT 0,
    message_type TEXT DEFAULT 'default',
    metadata TEXT  -- JSON blob for extra fields
);

CREATE TABLE IF NOT EXISTS checkpoints (
    channel_id TEXT PRIMARY KEY,
    last_message_id TEXT NOT NULL,
    last_fetch_timestamp TEXT NOT NULL,
    total_messages INTEGER DEFAULT 0,
    oldest_message_id TEXT,
    oldest_message_timestamp TEXT,
    newest_message_timestamp TEXT,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_messages_channel_timestamp 
    ON messages(channel_id, timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_messages_id 
    ON messages(message_id);
CREATE INDEX IF NOT EXISTS idx_checkpoints_channel 
    ON checkpoints(channel_id);
"""
```

**Key Decisions:**
- Why TEXT for IDs? Discord uses snowflake IDs (strings, not integers)
- Why timestamp DESC? We query recent messages most often
- Why JSON metadata? Flexibility for future fields without schema changes

#### Step 1.2: Implement MessageStorage Class

```python
import sqlite3
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from contextlib import contextmanager

class MessageStorage:
    def __init__(self, db_path: str = "data/raw_messages/messages.db"):
        self.db_path = db_path
        self.logger = logging.getLogger(__name__)
        self._ensure_db_directory()
        self._initialize_database()
    
    def _ensure_db_directory(self):
        """Create directory if it doesn't exist"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
    
    def _initialize_database(self):
        """Create tables and indexes if they don't exist"""
        with self._get_connection() as conn:
            try:
                conn.executescript(SCHEMA_SQL)
                conn.commit()
                self.logger.info("Database initialized successfully")
            except Exception as e:
                self.logger.error(f"Failed to initialize database: {e}")
                raise
    
    @contextmanager
    def _get_connection(self):
        """
        Context manager for database connections.
        
        Learning: Context managers ensure connections are always closed,
        even if exceptions occur. This prevents resource leaks.
        """
        conn = sqlite3.connect(self.db_path)
        try:
            yield conn
        finally:
            conn.close()
    
    def save_channel_messages(
        self, 
        channel_id: str, 
        messages: List[Dict], 
        is_incremental: bool = False
    ) -> bool:
        """
        Save messages to database using batch insert.
        
        Learning: Batch operations are much faster than individual inserts.
        Uses transaction for atomicity.
        """
        if not messages:
            return True
        
        with self._get_connection() as conn:
            try:
                cursor = conn.cursor()
                
                # Prepare batch insert
                insert_sql = """
                    INSERT OR IGNORE INTO messages (
                        message_id, channel_id, guild_id, content,
                        author_id, author_name, author_display_name,
                        channel_name, guild_name, timestamp, created_at,
                        is_bot, has_attachments, message_type, metadata
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                batch_data = []
                for msg in messages:
                    batch_data.append((
                        str(msg.get('id', '')),
                        str(channel_id),
                        str(msg.get('guild_id', '')),
                        msg.get('content', ''),
                        str(msg.get('author_id', '')),
                        msg.get('author', ''),
                        msg.get('author_display_name', ''),
                        msg.get('channel_name', ''),
                        msg.get('guild_name', ''),
                        msg.get('timestamp', ''),
                        msg.get('created_at', datetime.now().isoformat()),
                        1 if msg.get('is_bot', False) else 0,
                        1 if msg.get('has_attachments', False) else 0,
                        msg.get('message_type', 'default'),
                        json.dumps(msg.get('metadata', {}))
                    ))
                
                # Batch insert (much faster than individual inserts)
                cursor.executemany(insert_sql, batch_data)
                
                # Update checkpoint
                if messages:
                    newest_msg = max(messages, key=lambda m: m.get('timestamp', ''))
                    oldest_msg = min(messages, key=lambda m: m.get('timestamp', ''))
                    
                    self._update_checkpoint(
                        conn, channel_id, 
                        newest_msg.get('id', ''), 
                        newest_msg.get('timestamp', ''),
                        len(messages),
                        oldest_msg.get('id', ''),
                        oldest_msg.get('timestamp', ''),
                        newest_msg.get('timestamp', '')
                    )
                
                conn.commit()
                self.logger.info(f"Saved {len(messages)} messages for channel {channel_id}")
                return True
                
            except Exception as e:
                conn.rollback()
                self.logger.error(f"Error saving messages: {e}")
                return False
    
    def load_channel_messages(self, channel_id: str) -> List[Dict]:
        """
        Load all messages for a channel.
        
        Learning: Loads all messages (for chunking entire history).
        """
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT message_id, channel_id, guild_id, content,
                       author_id, author_name, author_display_name,
                       channel_name, guild_name, timestamp, created_at,
                       is_bot, has_attachments, message_type, metadata
                FROM messages
                WHERE channel_id = ?
                ORDER BY timestamp ASC
            """, (str(channel_id),))
            
            rows = cursor.fetchall()
            messages = []
            for row in rows:
                messages.append({
                    'id': row[0],
                    'channel_id': row[1],
                    'guild_id': row[2],
                    'content': row[3],
                    'author_id': row[4],
                    'author': row[5],
                    'author_display_name': row[6],
                    'channel_name': row[7],
                    'guild_name': row[8],
                    'timestamp': row[9],
                    'created_at': row[10],
                    'is_bot': bool(row[11]),
                    'has_attachments': bool(row[12]),
                    'message_type': row[13],
                    'metadata': json.loads(row[14]) if row[14] else {}
                })
            
            return messages
    
    def get_recent_messages(self, channel_id: str, limit: int = 50) -> List[Dict]:
        """
        Get most recent messages for a channel.
        
        Learning: Indexed query (channel_id + timestamp DESC) is fast.
        """
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT message_id, channel_id, guild_id, content,
                       author_id, author_name, author_display_name,
                       channel_name, guild_name, timestamp, created_at,
                       is_bot, has_attachments, message_type, metadata
                FROM messages
                WHERE channel_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
            """, (str(channel_id), limit))
            
            rows = cursor.fetchall()
            messages = []
            for row in rows:
                messages.append({
                    'id': row[0],
                    'channel_id': row[1],
                    'guild_id': row[2],
                    'content': row[3],
                    'author_id': row[4],
                    'author': row[5],
                    'author_display_name': row[6],
                    'channel_name': row[7],
                    'guild_name': row[8],
                    'timestamp': row[9],
                    'created_at': row[10],
                    'is_bot': bool(row[11]),
                    'has_attachments': bool(row[12]),
                    'message_type': row[13],
                    'metadata': json.loads(row[14]) if row[14] else {}
                })
            
            return messages
    
    def get_checkpoint(self, channel_id: str) -> Optional[Dict]:
        """Get checkpoint for a channel"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT channel_id, last_message_id, last_fetch_timestamp,
                       total_messages, oldest_message_id, oldest_message_timestamp,
                       newest_message_timestamp
                FROM checkpoints
                WHERE channel_id = ?
            """, (str(channel_id),))
            
            row = cursor.fetchone()
            if row:
                return {
                    'channel_id': row[0],
                    'last_message_id': row[1],
                    'last_fetch_timestamp': row[2],
                    'total_messages': row[3],
                    'oldest_message_id': row[4],
                    'oldest_message_timestamp': row[5],
                    'newest_message_timestamp': row[6]
                }
            return None
    
    def update_checkpoint(
        self, 
        channel_id: str, 
        last_message_id: str,
        timestamp: str,
        total_messages: int = None,
        oldest_message_id: str = None,
        oldest_timestamp: str = None,
        newest_timestamp: str = None
    ):
        """Update checkpoint atomically"""
        with self._get_connection() as conn:
            # Get current stats if not provided
            if total_messages is None:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT COUNT(*), MIN(timestamp), MAX(timestamp), 
                           MIN(message_id), MAX(message_id)
                    FROM messages
                    WHERE channel_id = ?
                """, (str(channel_id),))
                row = cursor.fetchone()
                if row:
                    total_messages = row[0] or 0
                    oldest_timestamp = row[1] or timestamp
                    newest_timestamp = row[2] or timestamp
                    oldest_message_id = row[3] or last_message_id
            
            self._update_checkpoint(
                conn, channel_id, last_message_id, timestamp,
                total_messages, oldest_message_id or last_message_id,
                oldest_timestamp or timestamp, newest_timestamp or timestamp
            )
            conn.commit()
    
    def _update_checkpoint(self, conn, channel_id, last_message_id, 
                          timestamp, total_messages, oldest_message_id,
                          oldest_timestamp, newest_timestamp):
        """Internal method to update checkpoint"""
        cursor = conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO checkpoints (
                channel_id, last_message_id, last_fetch_timestamp,
                total_messages, oldest_message_id, oldest_message_timestamp,
                newest_message_timestamp, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            str(channel_id), str(last_message_id), timestamp,
            total_messages, str(oldest_message_id), oldest_timestamp,
            newest_timestamp, datetime.now().isoformat()
        ))
    
    def list_stored_channels(self) -> List[Dict]:
        """List all channels with stored messages"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT channel_id, total_messages, newest_message_timestamp
                FROM checkpoints
                ORDER BY newest_message_timestamp DESC
            """)
            
            channels = []
            for row in cursor.fetchall():
                channels.append({
                    'channel_id': row[0],
                    'total_messages': row[1],
                    'newest_message_timestamp': row[2]
                })
            return channels
    
    def get_channel_stats(self, channel_id: str) -> Dict:
        """Get statistics for a channel"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # Get message count and date range
            cursor.execute("""
                SELECT COUNT(*), MIN(timestamp), MAX(timestamp)
                FROM messages
                WHERE channel_id = ?
            """, (str(channel_id),))
            
            row = cursor.fetchone()
            checkpoint = self.get_checkpoint(channel_id)
            
            return {
                'channel_id': channel_id,
                'message_count': row[0] if row else 0,
                'oldest_timestamp': row[1] if row and row[1] else None,
                'newest_timestamp': row[2] if row and row[2] else None,
                'checkpoint': checkpoint
            }
```

**Key Learning Points:**
- **Context Managers**: `@contextmanager` ensures connections are always closed
- **Batch Operations**: `executemany()` is much faster than individual inserts
- **Transactions**: `conn.commit()` ensures atomicity (all or nothing)
- **Indexes**: Queries on indexed columns are fast (channel_id + timestamp)
- **ON CONFLICT IGNORE**: Prevents duplicate inserts (idempotent)

#### Step 1.3: Testing

Create a test script `test_message_storage.py`:

```python
import asyncio
from services.message_storage import MessageStorage
from utils.discord_utils import format_discord_message

def test_message_storage():
    storage = MessageStorage("test_messages.db")
    
    # Create test messages
    test_messages = [
        {
            'id': '123',
            'content': 'Hello world',
            'author_id': '456',
            'author': 'TestUser',
            'author_display_name': 'TestUser',
            'timestamp': '2024-01-01T00:00:00Z',
            'created_at': '2024-01-01T00:00:00Z',
            'channel_name': 'test-channel',
            'guild_name': 'Test Guild',
            'guild_id': '789',
            'is_bot': False,
            'has_attachments': False,
            'message_type': 'default',
            'metadata': {}
        },
        {
            'id': '124',
            'content': 'Second message',
            'author_id': '456',
            'author': 'TestUser',
            'author_display_name': 'TestUser',
            'timestamp': '2024-01-01T00:01:00Z',
            'created_at': '2024-01-01T00:01:00Z',
            'channel_name': 'test-channel',
            'guild_name': 'Test Guild',
            'guild_id': '789',
            'is_bot': False,
            'has_attachments': False,
            'message_type': 'default',
            'metadata': {}
        }
    ]
    
    # Test save
    result = storage.save_channel_messages('test_channel', test_messages)
    assert result == True
    
    # Test retrieve
    messages = storage.get_recent_messages('test_channel', limit=10)
    assert len(messages) == 2
    assert messages[0]['id'] == '124'  # Most recent first
    
    # Test load all
    all_messages = storage.load_channel_messages('test_channel')
    assert len(all_messages) == 2
    
    # Test checkpoint
    checkpoint = storage.get_checkpoint('test_channel')
    assert checkpoint is not None
    assert checkpoint['last_message_id'] == '124'
    assert checkpoint['total_messages'] == 2
    
    # Test idempotency (duplicate messages)
    storage.save_channel_messages('test_channel', test_messages)
    all_messages_after = storage.load_channel_messages('test_channel')
    assert len(all_messages_after) == 2  # No duplicates
    
    # Test stats
    stats = storage.get_channel_stats('test_channel')
    assert stats['message_count'] == 2
    
    print("âœ… All tests passed!")

if __name__ == "__main__":
    test_message_storage()
```

**What to Test:**
- Messages save correctly
- Checkpoint updates after save
- Recent messages query works (newest first)
- Load all messages works (oldest first)
- Duplicate messages are ignored (idempotent)
- Channel stats are accurate

### Common Pitfalls - Phase 1

1. **Forgetting to close connections**: Always use context managers
2. **Not handling empty results**: Check if `row` is None before accessing
3. **String vs int IDs**: Discord IDs are strings, not integers
4. **Timestamp ordering**: DESC for recent, ASC for chronological
5. **Missing indexes**: Queries will be slow without proper indexes

### Debugging Tips - Phase 1

- **Check database file**: Use SQLite browser to inspect data
- **Log queries**: Add logging to see what SQL is executed
- **Test with small data**: Start with 10 messages before scaling
- **Verify indexes**: Use `EXPLAIN QUERY PLAN` to check index usage

### Performance Considerations - Phase 1

- **Batch inserts**: 100x faster than individual inserts
- **Indexes**: Critical for fast queries on large datasets
- **Connection pooling**: SQLite doesn't need it, but PostgreSQL would
- **Transaction size**: Don't commit after every message (batch!)

---

## Phase 2: Rate Limiting & API Design

### Learning Objectives
- Understand Discord API rate limits
- Implement exponential backoff
- Design progress reporting
- Learn async error handling

### Design Principles
- **Configurable Behavior**: Rate limits as configuration
- **Error Recovery**: Graceful handling of rate limit errors
- **Progress Reporting**: Observer pattern for progress updates

### Implementation Steps

#### Step 2.1: Add Rate Limiting Configuration

Update `config.py`:

```python
# Rate Limiting Configuration
MESSAGE_FETCH_DELAY: float = float(os.getenv("MESSAGE_FETCH_DELAY", "1.0"))
MESSAGE_FETCH_BATCH_SIZE: int = int(os.getenv("MESSAGE_FETCH_BATCH_SIZE", "100"))
MESSAGE_FETCH_PROGRESS_INTERVAL: int = int(os.getenv("MESSAGE_FETCH_PROGRESS_INTERVAL", "100"))
MESSAGE_FETCH_MAX_RETRIES: int = int(os.getenv("MESSAGE_FETCH_MAX_RETRIES", "5"))
```

#### Step 2.2: Enhance MessageLoader with Rate Limiting

Modify `bot/loaders/message_loader.py`:

```python
import asyncio
import logging
from datetime import datetime
from typing import Callable, Optional, Dict, Any
from discord import HTTPException
from config import Config
from rag import MemoryService
from bot.utils.discord_utils import format_discord_message

class MessageLoader:
    def __init__(self, memory_service: MemoryService = None):
        self.memory_service = memory_service
        self.logger = logging.getLogger(__name__)
        self.rate_limit_delay = Config.MESSAGE_FETCH_DELAY
        self.progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None
    
    def set_progress_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """
        Set callback for progress updates (Observer pattern).
        
        Learning: Callbacks enable decoupled progress reporting.
        """
        self.progress_callback = callback
    
    async def _rate_limit_delay(self):
        """Wait between requests to respect rate limits"""
        await asyncio.sleep(self.rate_limit_delay)
    
    async def _handle_rate_limit_error(
        self, 
        error: HTTPException, 
        retry_count: int
    ) -> bool:
        """
        Handle 429 (rate limit) errors with exponential backoff.
        
        Learning: Exponential backoff prevents overwhelming the API.
        """
        if error.status == 429:  # Too Many Requests
            retry_after = getattr(error, 'retry_after', None) or (2 ** retry_count)
            wait_time = min(retry_after, 60)  # Cap at 60 seconds
            
            self.logger.warning(
                f"Rate limited! Waiting {wait_time}s before retry "
                f"(attempt {retry_count + 1}/{Config.MESSAGE_FETCH_MAX_RETRIES})"
            )
            
            await asyncio.sleep(wait_time)
            return True  # Retry
        return False  # Don't retry
    
    async def load_channel_messages(
        self,
        channel,
        limit: Optional[int] = None,
        before: Optional = None,
        after: Optional = None,
        rate_limit_delay: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Load messages with rate limiting and progress reporting.
        
        Learning: 
        - Rate limiting prevents API bans
        - Progress callbacks enable UI updates
        - Error recovery with exponential backoff
        """
        if rate_limit_delay:
            self.rate_limit_delay = rate_limit_delay
        
        stats = {
            "total_processed": 0,
            "successfully_loaded": 0,
            "skipped_bot_messages": 0,
            "skipped_empty_messages": 0,
            "skipped_blacklisted": 0,
            "skipped_commands": 0,
            "errors": 0,
            "rate_limit_errors": 0,
            "start_time": datetime.now(),
            "end_time": None,
        }
        
        try:
            retry_count = 0
            
            async for message in channel.history(
                limit=limit, 
                before=before, 
                after=after, 
                oldest_first=False
            ):
                try:
                    # Process message
                    stats["total_processed"] += 1
                    
                    # Skip messages that shouldn't be stored
                    if message.author.bot:
                        stats["skipped_bot_messages"] += 1
                        await self._rate_limit_delay()  # Still respect rate limit
                        continue
                    elif message.author.id in Config.BLACKLIST_IDS:
                        stats["skipped_blacklisted"] += 1
                        await self._rate_limit_delay()
                        continue
                    elif not message.content.strip():
                        stats["skipped_empty_messages"] += 1
                        await self._rate_limit_delay()
                        continue
                    elif message.content.startswith(Config.BOT_PREFIX):
                        stats["skipped_commands"] += 1
                        await self._rate_limit_delay()
                        continue
                    
                    # Store message if memory service provided
                    if self.memory_service:
                        message_data = format_discord_message(message)
                        success = await self.memory_service.store_message(message_data)
                        if success:
                            stats["successfully_loaded"] += 1
                        else:
                            stats["errors"] += 1
                    
                    # Rate limit delay between messages
                    await self._rate_limit_delay()
                    
                    # Progress reporting
                    if stats["total_processed"] % Config.MESSAGE_FETCH_PROGRESS_INTERVAL == 0:
                        if self.progress_callback:
                            elapsed = (datetime.now() - stats["start_time"]).total_seconds()
                            rate = stats["total_processed"] / elapsed if elapsed > 0 else 0
                            progress = {
                                "processed": stats["total_processed"],
                                "limit": limit or "unlimited",
                                "rate": rate,
                                "successful": stats["successfully_loaded"]
                            }
                            if asyncio.iscoroutinefunction(self.progress_callback):
                                await self.progress_callback(progress)
                            else:
                                self.progress_callback(progress)
                    
                    # Reset retry count on success
                    retry_count = 0
                    
                except HTTPException as e:
                    # Handle rate limit errors
                    if await self._handle_rate_limit_error(e, retry_count):
                        stats["rate_limit_errors"] += 1
                        retry_count += 1
                        
                        if retry_count >= Config.MESSAGE_FETCH_MAX_RETRIES:
                            self.logger.error("Max retries exceeded")
                            break
                        continue
                    else:
                        stats["errors"] += 1
                        self.logger.error(f"HTTP error: {e}")
                
                except Exception as e:
                    stats["errors"] += 1
                    self.logger.error(f"Error processing message: {e}")
            
            stats["end_time"] = datetime.now()
            return stats
            
        except Exception as e:
            self.logger.error(f"Fatal error loading messages: {e}")
            stats["end_time"] = datetime.now()
            stats["errors"] += 1
            return stats
```

**Key Learning Points:**
- **Rate Limiting**: `await asyncio.sleep()` between requests
- **Exponential Backoff**: `2^retry_count` seconds, capped at 60s
- **Observer Pattern**: Progress callback for UI updates
- **Error Recovery**: Retry on rate limit, fail on other errors
- **Async Callbacks**: Check if callback is coroutine function

#### Step 2.3: Test Rate Limiting

Test with a medium dataset (1000 messages):

```python
async def test_rate_limiting():
    from services.message_loader import MessageLoader
    
    loader = MessageLoader()
    
    # Set progress callback
    async def on_progress(progress):
        print(f"Progress: {progress['processed']} messages, "
              f"Rate: {progress['rate']:.2f} msg/s")
    
    loader.set_progress_callback(on_progress)
    
    # Test with 1000 messages
    stats = await loader.load_channel_messages(channel, limit=1000)
    
    # Verify rate limiting worked
    duration = (stats['end_time'] - stats['start_time']).total_seconds()
    avg_rate = stats['total_processed'] / duration
    assert avg_rate < 2.0  # Should be around 1 msg/s
    
    print(f"âœ… Processed {stats['total_processed']} messages in {duration:.1f}s")
    print(f"   Average rate: {avg_rate:.2f} msg/s")
```

### Common Pitfalls - Phase 2

1. **Forgetting await**: `asyncio.sleep()` must be awaited
2. **Not checking callback type**: Use `iscoroutinefunction()` for async callbacks
3. **Rate limit too low**: < 0.5s will get you banned
4. **Not handling retry_after**: Discord tells you how long to wait
5. **Progress updates too frequent**: Can slow down fetching

### Debugging Tips - Phase 2

- **Monitor rate**: Log actual request rate
- **Check retry_after**: Discord sends this in 429 responses
- **Test with small limits**: Start with 100 messages
- **Watch for bans**: If you get 403, you're banned temporarily

### Performance Considerations - Phase 2

- **Rate limit delay**: 1.0s is safe, 0.5s is risky
- **Progress updates**: Every 100 messages is good balance
- **Batch size**: Not used here, but useful for processing

---

## Phase 3: Embedding Service Abstraction

### Learning Objectives
- Learn abstraction layer design
- Understand Strategy pattern
- Practice dependency injection
- Compare local vs cloud embeddings

### Design Principles
- **Strategy Pattern**: Different implementations, same interface
- **Dependency Inversion**: Depend on abstractions, not concretions
- **Factory Pattern**: Create objects from configuration

### Implementation Steps

#### Step 3.1: Create Abstract Base Class

Create `embedding/base.py`:

```python
from abc import ABC, abstractmethod
from typing import List

class EmbeddingProvider(ABC):
    """
    Abstract base class for embedding providers.
    
    Learning: Abstract classes define contracts that implementations must follow.
    This enables swapping implementations without changing business logic.
    """
    
    @abstractmethod
    def encode(self, text: str) -> List[float]:
        """
        Encode a single text into an embedding vector.
        
        Returns:
            List of floats representing the embedding
        """
        pass
    
    @abstractmethod
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Encode multiple texts into embedding vectors.
        
        Learning: Batch operations are often more efficient.
        
        Returns:
            List of embedding vectors
        """
        pass
    
    @property
    @abstractmethod
    def dimension(self) -> int:
        """Return the dimension of the embedding vectors"""
        pass
```

**Key Learning Points:**
- **ABC**: Python's Abstract Base Class enforces interface
- **Abstract Methods**: Must be implemented by subclasses
- **Interface Design**: Methods that all providers must support

#### Step 3.2: Implement SentenceTransformer Provider

Create `embedding/sentence_transformer.py`:

```python
from embedding.base import EmbeddingProvider
from sentence_transformers import SentenceTransformer
import logging

class SentenceTransformerEmbedder(EmbeddingProvider):
    """
    Local embedding provider using sentence-transformers.
    
    Learning: Adapter pattern - wraps existing library with our interface.
    """
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        self.logger.info(f"Loading sentence transformer model: {model_name}")
        self._model = SentenceTransformer(model_name)
        self._dimension = self._model.get_sentence_embedding_dimension()
    
    def encode(self, text: str) -> List[float]:
        """Encode single text"""
        embedding = self._model.encode(text, convert_to_numpy=True)
        return embedding.tolist()
    
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Encode batch of texts.
        
        Learning: Batch encoding is more efficient than individual calls.
        """
        embeddings = self._model.encode(
            texts, 
            convert_to_numpy=True,
            batch_size=32,  # Process in batches
            show_progress_bar=False
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self._dimension
```

#### Step 3.3: Implement OpenAI Provider

Create `embedding/openai.py`:

```python
from embedding.base import EmbeddingProvider
import openai
from typing import List
import logging
import asyncio
from config import Config

class OpenAIEmbedder(EmbeddingProvider):
    """
    Cloud embedding provider using OpenAI API.
    
    Learning: 
    - API clients need error handling and retries
    - Cost tracking is important for cloud services
    """
    
    def __init__(self, model_name: str = "text-embedding-3-small"):
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        if not Config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not configured")
        self.client = openai.OpenAI(api_key=Config.OPENAI_API_KEY)
        
        # Dimension mapping for different models
        self._dimensions = {
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072,
            "text-embedding-ada-002": 1536
        }
        self._dimension = self._dimensions.get(model_name, 1536)
    
    def encode(self, text: str) -> List[float]:
        """Encode single text with retry logic"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=text
                )
                return response.data[0].embedding
            except Exception as e:
                if attempt == max_retries - 1:
                    self.logger.error(f"Embedding failed after {max_retries} attempts: {e}")
                    raise
                self.logger.warning(f"Embedding failed, retrying ({attempt + 1}/{max_retries}): {e}")
                # Wait before retry (synchronous, not async)
                import time
                time.sleep(2 ** attempt)  # Exponential backoff
    
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Encode batch of texts.
        
        Learning: OpenAI API supports batch encoding natively.
        """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=texts
                )
                return [item.embedding for item in response.data]
            except Exception as e:
                if attempt == max_retries - 1:
                    self.logger.error(f"Batch embedding failed after {max_retries} attempts: {e}")
                    raise
                self.logger.warning(f"Batch embedding failed, retrying ({attempt + 1}/{max_retries}): {e}")
                import time
                time.sleep(2 ** attempt)
```

#### Step 3.4: Create Factory

Create `embedding/factory.py`:

```python
from typing import Optional
from config import Config
from embedding.base import EmbeddingProvider
from embedding.sentence_transformer import SentenceTransformerEmbedder
from embedding.openai import OpenAIEmbedder

class EmbeddingFactory:
    """
    Factory for creating embedding providers.
    
    Learning: Factory pattern centralizes object creation logic.
    Makes it easy to switch providers via configuration.
    """
    
    @staticmethod
    def create(provider_name: Optional[str] = None) -> EmbeddingProvider:
        """
        Create embedding provider based on configuration.
        
        Learning: Configuration-driven creation enables runtime switching.
        """
        provider_name = provider_name or Config.EMBEDDING_PROVIDER
        
        if provider_name == "sentence-transformers":
            return SentenceTransformerEmbedder()
        elif provider_name == "openai":
            return OpenAIEmbedder()
        else:
            raise ValueError(f"Unknown embedding provider: {provider_name}")
```

**Key Learning Points:**
- **Strategy Pattern**: Same interface, different implementations
- **Factory Pattern**: Centralized creation logic
- **Dependency Injection**: Pass provider to services, don't create internally

### Common Pitfalls - Phase 3

1. **Async/await confusion**: OpenAI client is synchronous, not async
2. **Missing API key**: Check config before creating client
3. **Dimension mismatch**: Different models have different dimensions
4. **Batch size too large**: Can hit API limits
5. **No retry logic**: API can be flaky, need retries

### Debugging Tips - Phase 3

- **Test dimensions**: Verify embedding dimensions match
- **Check API keys**: Test with simple encode first
- **Monitor costs**: OpenAI charges per token
- **Compare outputs**: Test both providers with same text

### Performance Considerations - Phase 3

- **Local vs Cloud**: Local is free but slower, cloud is faster but costs
- **Batch size**: 32-64 is good for sentence-transformers
- **Model size**: Smaller models are faster but less accurate

---

## Phase 4: Chunking Strategies

### Learning Objectives
- Understand different chunking approaches
- Learn when to use each strategy
- Design extensible chunking system
- Practice algorithm design

### Design Principles
- **Strategy Pattern**: Different chunking algorithms
- **Configurable Parameters**: Window sizes, gaps as config
- **Extensibility**: Easy to add new strategies

### Implementation Steps

#### Step 4.1: Design Chunk Data Structure

Create `chunking/base.py` for the Chunk class and `chunking/service.py` for ChunkingService:

```python
from typing import List, Dict, Optional
from datetime import datetime
from config import Config

class Chunk:
    """
    Data structure for a chunk.
    
    Learning: Good data modeling is crucial for RAG systems.
    Metadata enables filtering and context preservation.
    """
    def __init__(
        self,
        content: str,
        message_ids: List[str],
        metadata: Dict
    ):
        self.content = content
        self.message_ids = message_ids
        self.metadata = metadata
    
    def to_dict(self) -> Dict:
        """Convert chunk to dictionary for storage"""
        return {
            "content": self.content,
            "message_ids": self.message_ids,
            "metadata": self.metadata
        }

class ChunkingService:
    """
    Service for chunking messages using different strategies.
    
    Learning: Strategy pattern allows experimenting with different approaches.
    """
    
    def __init__(
        self,
        temporal_window: int = None,
        conversation_gap: int = None
    ):
        self.temporal_window = temporal_window or Config.CHUNKING_TEMPORAL_WINDOW
        self.conversation_gap = conversation_gap or Config.CHUNKING_CONVERSATION_GAP
```

#### Step 4.2: Implement Temporal Chunking

```python
def chunk_temporal(self, messages: List[Dict]) -> List[Chunk]:
    """
    Group messages by time windows.
    
    Learning: Temporal chunking preserves time-based context.
    Useful for conversations that happen over time.
    """
    if not messages:
        return []
    
    # Sort messages by timestamp
    sorted_messages = sorted(
        messages, 
        key=lambda m: m.get('timestamp', '')
    )
    
    chunks = []
    current_chunk = []
    window_start = None
    
    for message in sorted_messages:
        try:
            # Parse timestamp
            timestamp_str = message.get('timestamp', '')
            if 'Z' in timestamp_str:
                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            else:
                timestamp = datetime.fromisoformat(timestamp_str)
        except Exception:
            # Skip messages with invalid timestamps
            continue
        
        # Start new window if needed
        if window_start is None:
            window_start = timestamp
        
        # Check if message is outside current window
        time_diff = (timestamp - window_start).total_seconds()
        
        if time_diff > self.temporal_window:
            # Save current chunk and start new one
            if current_chunk:
                chunks.append(self._create_chunk(current_chunk, "temporal"))
            current_chunk = [message]
            window_start = timestamp
        else:
            current_chunk.append(message)
    
    # Don't forget last chunk
    if current_chunk:
        chunks.append(self._create_chunk(current_chunk, "temporal"))
    
    return chunks
```

#### Step 4.3: Implement Conversation Chunking

```python
def chunk_conversation(self, messages: List[Dict]) -> List[Chunk]:
    """
    Group messages by conversation boundaries.
    
    Learning: Conversation chunking detects natural breaks in dialogue.
    Boundaries: time gaps, channel changes, topic shifts.
    """
    if not messages:
        return []
    
    sorted_messages = sorted(
        messages,
        key=lambda m: m.get('timestamp', '')
    )
    
    chunks = []
    current_chunk = []
    last_timestamp = None
    
    for message in sorted_messages:
        try:
            timestamp_str = message.get('timestamp', '')
            if 'Z' in timestamp_str:
                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            else:
                timestamp = datetime.fromisoformat(timestamp_str)
        except Exception:
            continue
        
        channel_id = message.get('channel_id')
        
        # Check for conversation boundary
        is_boundary = False
        
        if last_timestamp:
            time_gap = (timestamp - last_timestamp).total_seconds()
            if time_gap > self.conversation_gap:
                is_boundary = True
        
        # Channel change is also a boundary
        if current_chunk and current_chunk[-1].get('channel_id') != channel_id:
            is_boundary = True
        
        if is_boundary and current_chunk:
            chunks.append(self._create_chunk(current_chunk, "conversation"))
            current_chunk = [message]
        else:
            current_chunk.append(message)
        
        last_timestamp = timestamp
    
    if current_chunk:
        chunks.append(self._create_chunk(current_chunk, "conversation"))
    
    return chunks
```

#### Step 4.4: Implement Single Message Chunking

```python
def chunk_single(self, messages: List[Dict]) -> List[Chunk]:
    """
    Each message is its own chunk.
    
    Learning: Baseline strategy for comparison.
    Maximum granularity, preserves all individual messages.
    """
    chunks = []
    for message in messages:
        chunks.append(self._create_chunk([message], "single"))
    return chunks

def _create_chunk(self, messages: List[Dict], strategy: str) -> Chunk:
    """Helper to create chunk with metadata"""
    if not messages:
        raise ValueError("Cannot create chunk from empty messages")
    
    # Format content
    content_parts = []
    for msg in messages:
        author = msg.get('author_display_name') or msg.get('author', 'Unknown')
        timestamp = msg.get('timestamp', '')[:10]  # Date only
        content = msg.get('content', '').strip()
        if content:
            content_parts.append(f"{timestamp} - {author}: {content}")
    
    content = "\n".join(content_parts)
    
    # Collect metadata
    message_ids = [str(msg.get('id', '')) for msg in messages if msg.get('id')]
    metadata = {
        "chunk_strategy": strategy,
        "channel_id": messages[0].get('channel_id', ''),
        "message_count": len(messages),
        "first_message_id": message_ids[0] if message_ids else '',
        "last_message_id": message_ids[-1] if message_ids else '',
        "first_timestamp": messages[0].get('timestamp', ''),
        "last_timestamp": messages[-1].get('timestamp', ''),
    }
    
    return Chunk(content, message_ids, metadata)
```

#### Step 4.5: Main Chunking Method

```python
def chunk_messages(
    self, 
    messages: List[Dict], 
    strategies: List[str] = None
) -> Dict[str, List[Chunk]]:
    """
    Generate chunks using specified strategies.
    
    Learning: Returns all strategies at once for comparison.
    """
    if strategies is None:
        strategies = ["temporal", "conversation", "single"]
    
    results = {}
    
    if "temporal" in strategies:
        results["temporal"] = self.chunk_temporal(messages)
    
    if "conversation" in strategies:
        results["conversation"] = self.chunk_conversation(messages)
    
    if "single" in strategies:
        results["single"] = self.chunk_single(messages)
    
    return results
```

### Common Pitfalls - Phase 4

1. **Timestamp parsing**: Handle both ISO formats (with/without Z)
2. **Empty chunks**: Don't create chunks with no messages
3. **Sorting**: Always sort by timestamp before chunking
4. **Boundary detection**: Time gaps must account for timezone
5. **Metadata**: Include all info needed for filtering later

### Debugging Tips - Phase 4

- **Print chunk sizes**: See how many messages per chunk
- **Check timestamps**: Verify they're parsed correctly
- **Test boundaries**: Create test data with known gaps
- **Compare strategies**: Visualize chunk boundaries

### Performance Considerations - Phase 4

- **Sorting**: O(n log n) complexity, but necessary
- **Chunk count**: More chunks = more embeddings = more storage
- **Content length**: Keep chunks under token limits

---

## Phase 5: Vector Store Abstraction

### Learning Objectives
- Learn adapter pattern in practice
- Understand abstraction layers
- Compare cloud vs local storage
- Design for multi-provider support

### Design Principles
- **Adapter Pattern**: Wrap different APIs with same interface
- **Dependency Inversion**: Depend on abstractions
- **Provider Pattern**: Switch implementations easily

### Implementation Steps

#### Step 5.1: Create Abstract Base Class

Create `retrieval/base.py`:

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Optional

class VectorStore(ABC):
    """
    Abstract interface for vector stores.
    
    Learning: This abstraction allows swapping ChromaDB, Pinecone, etc.
    without changing business logic.
    """
    
    @abstractmethod
    def create_collection(self, name: str, metadata: Dict = None):
        """Create a new collection"""
        pass
    
    @abstractmethod
    def get_collection(self, name: str):
        """Get existing collection"""
        pass
    
    @abstractmethod
    def add_documents(
        self,
        collection_name: str,
        documents: List[str],
        embeddings: List[List[float]],
        metadatas: List[Dict],
        ids: List[str]
    ):
        """
        Add documents to collection.
        
        Learning: Batch operations are more efficient.
        """
        pass
    
    @abstractmethod
    def query(
        self,
        collection_name: str,
        query_embeddings: List[List[float]],
        n_results: int,
        where: Dict = None
    ) -> Dict:
        """
        Query collection by similarity.
        
        Returns:
            {
                'documents': [[text1, text2, ...]],
                'metadatas': [[meta1, meta2, ...]],
                'distances': [[dist1, dist2, ...]]
            }
        """
        pass
    
    @abstractmethod
    def get_collection_count(self, collection_name: str) -> int:
        """Get number of documents in collection"""
        pass
    
    @abstractmethod
    def list_collections(self) -> List[str]:
        """List all collection names"""
        pass
```

#### Step 5.2: Implement ChromaDB Adapter

Create `retrieval/providers/chroma.py`:

```python
from retrieval.base import VectorStore
from data.chroma_client import chroma_client
from typing import List, Dict
import logging

class ChromaVectorStore(VectorStore):
    """
    ChromaDB implementation of vector store.
    
    Learning: Adapter pattern - wraps ChromaDB with our interface.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.client = chroma_client.client
    
    def create_collection(self, name: str, metadata: Dict = None):
        """Create collection (ChromaDB auto-creates on get)"""
        return self.client.get_or_create_collection(
            name=name,
            metadata=metadata or {}
        )
    
    def get_collection(self, name: str):
        """Get collection"""
        try:
            return self.client.get_collection(name)
        except Exception:
            # Collection doesn't exist, create it
            return self.create_collection(name)
    
    def add_documents(
        self,
        collection_name: str,
        documents: List[str],
        embeddings: List[List[float]],
        metadatas: List[Dict],
        ids: List[str]
    ):
        """Add documents in batch"""
        collection = self.get_collection(collection_name)
        collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )
    
    def query(
        self,
        collection_name: str,
        query_embeddings: List[List[float]],
        n_results: int,
        where: Dict = None
    ) -> Dict:
        """Query collection"""
        collection = self.get_collection(collection_name)
        results = collection.query(
            query_embeddings=query_embeddings,
            n_results=n_results,
            where=where
        )
        return results
    
    def get_collection_count(self, collection_name: str) -> int:
        """Get document count"""
        try:
            collection = self.get_collection(collection_name)
            return collection.count()
        except Exception:
            return 0
    
    def list_collections(self) -> List[str]:
        """List collections"""
        try:
            return [col.name for col in self.client.list_collections()]
        except Exception:
            return []
```

#### Step 5.3: Create Factory

Create `retrieval/factory.py`:

```python
from typing import Optional
from config import Config
from retrieval.base import VectorStore
from retrieval.providers.chroma import ChromaVectorStore

class VectorStoreFactory:
    """Factory for creating vector store instances"""
    
    @staticmethod
    def create(provider_name: Optional[str] = None) -> VectorStore:
        """
        Create vector store based on configuration.
        
        Learning: Factory pattern enables easy provider switching.
        """
        provider_name = provider_name or Config.VECTOR_STORE_PROVIDER
        
        if provider_name == "chroma":
            return ChromaVectorStore()
        elif provider_name == "pinecone":
            # Future: return PineconeVectorStore()
            raise NotImplementedError("Pinecone not yet implemented")
        else:
            raise ValueError(f"Unknown vector store: {provider_name}")
```

### Common Pitfalls - Phase 5

1. **Collection not found**: Handle missing collections gracefully
2. **Dimension mismatch**: Embeddings must match collection dimension
3. **Metadata types**: ChromaDB requires specific types
4. **Query format**: Results structure differs between providers

### Debugging Tips - Phase 5

- **Check collections**: List all collections to verify creation
- **Test queries**: Query with known documents first
- **Verify dimensions**: Ensure embedding dimensions match
- **Check metadata**: Ensure metadata is JSON-serializable

---

## Phase 6: Multi-Strategy Chunk Storage

### Learning Objectives
- Understand RAG vector store architecture
- Learn collection/namespace patterns
- Design for experimentation
- Compare strategies

### Implementation Steps

#### Step 6.1: Create ChunkedMemoryService

Create `rag/memory_service.py`:

```python
from retrieval.base import VectorStore
from embedding.base import EmbeddingProvider
from typing import List, Dict, Optional
import logging

class ChunkedMemoryService:
    """
    Manages chunk storage with multiple strategies.
    
    Learning: RAG architecture - store once, query multiple ways.
    """
    
    def __init__(
        self,
        vector_store: VectorStore,
        embedding_provider: EmbeddingProvider
    ):
        self.vector_store = vector_store
        self.embedding_provider = embedding_provider
        self.logger = logging.getLogger(__name__)
        self.active_strategy = "temporal"  # Default
    
    def store_all_strategies(
        self,
        chunks_dict: Dict[str, List]  # {"temporal": [chunks], ...}
    ):
        """
        Store chunks for all strategies in separate collections.
        
        Learning: Separate collections enable strategy comparison.
        """
        for strategy, chunks in chunks_dict.items():
            if not chunks:
                self.logger.warning(f"No chunks for strategy {strategy}, skipping")
                continue
                
            collection_name = f"discord_chunks_{strategy}"
            
            # Create collection if needed
            self.vector_store.create_collection(collection_name)
            
            # Prepare batch data
            documents = [chunk.content for chunk in chunks]
            metadatas = [chunk.metadata for chunk in chunks]
            ids = [f"{strategy}_{i}_{chunk.metadata.get('first_message_id', i)}" 
                   for i, chunk in enumerate(chunks)]
            
            # Generate embeddings in batch
            self.logger.info(f"Generating embeddings for {len(chunks)} {strategy} chunks...")
            try:
                embeddings = self.embedding_provider.encode_batch(documents)
                
                # Verify dimensions match
                if embeddings and len(embeddings[0]) != self.embedding_provider.dimension:
                    raise ValueError(
                        f"Embedding dimension mismatch: "
                        f"expected {self.embedding_provider.dimension}, "
                        f"got {len(embeddings[0])}"
                    )
                
                # Store in vector DB
                self.vector_store.add_documents(
                    collection_name=collection_name,
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    ids=ids
                )
                
                self.logger.info(f"Stored {len(chunks)} chunks for {strategy} strategy")
            except Exception as e:
                self.logger.error(f"Error storing {strategy} chunks: {e}")
                raise
    
    def search(
        self,
        query: str,
        strategy: Optional[str] = None,
        top_k: int = 5
    ) -> List[Dict]:
        """
        Search using specified strategy.
        
        Learning: Strategy selection at query time enables comparison.
        """
        strategy = strategy or self.active_strategy
        collection_name = f"discord_chunks_{strategy}"
        
        # Generate query embedding
        query_embedding = self.embedding_provider.encode(query)
        
        # Query vector store
        results = self.vector_store.query(
            collection_name=collection_name,
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        # Format results
        formatted_results = []
        if results.get('documents') and results['documents'][0]:
            for i, doc in enumerate(results['documents'][0]):
                formatted_results.append({
                    'content': doc,
                    'metadata': results['metadatas'][0][i] if results.get('metadatas') else {},
                    'similarity': 1 - results['distances'][0][i] if results.get('distances') else 0.0
                })
        
        return formatted_results
    
    def get_strategy_stats(self) -> Dict[str, int]:
        """Get document counts per strategy"""
        stats = {}
        for strategy in ["temporal", "conversation", "single"]:
            collection_name = f"discord_chunks_{strategy}"
            try:
                stats[strategy] = self.vector_store.get_collection_count(collection_name)
            except Exception:
                stats[strategy] = 0
        return stats
    
    def switch_active_strategy(self, strategy: str):
        """Switch the active strategy for queries"""
        if strategy not in ["temporal", "conversation", "single"]:
            raise ValueError(f"Unknown strategy: {strategy}")
        self.active_strategy = strategy
        self.logger.info(f"Switched active strategy to {strategy}")
```

### Common Pitfalls - Phase 6

1. **Empty chunks**: Don't try to store empty chunk lists
2. **Dimension mismatch**: Verify embedding dimensions
3. **ID collisions**: Use unique IDs across strategies
4. **Missing collections**: Handle case where collection doesn't exist

---

## Phase 7: Bot Commands Integration

### Learning Objectives
- Integrate all components
- Design user-friendly commands
- Practice async progress reporting
- Learn component orchestration

### Implementation Steps

#### Step 7.1: Add Chunk Channel Command

Update `bot/cogs/admin.py`:

```python
from storage import MessageStorage
from bot.loaders.message_loader import MessageLoader
from chunking import ChunkingService
from rag import ChunkedMemoryService
from embedding import EmbeddingFactory
from retrieval import VectorStoreFactory
from bot.utils.discord_utils import format_discord_message
import discord
from discord.ext import commands
from config import Config

@commands.command(name='chunk_channel')
@commands.is_owner()
async def chunk_channel(
    self, 
    ctx, 
    limit: int = None,
    rate_limit_delay: float = None
):
    """
    Fetch, chunk, and store messages for a channel.
    
    Usage: !chunk_channel [limit] [--rate-limit-delay SECONDS]
    
    Learning: Component orchestration - bringing all pieces together.
    """
    # Manual owner check
    if str(ctx.author.id) != str(Config.BOT_OWNER_ID):
        await ctx.send("ðŸš« **Access Denied!** Only the bot owner can use this command.")
        return
    
    try:
        # Initialize services
        message_storage = MessageStorage()
        message_loader = MessageLoader()
        chunking_service = ChunkingService()
        
        # Initialize vector store and embedding provider
        vector_store = VectorStoreFactory.create()
        embedding_provider = EmbeddingServiceFactory.create()
        chunked_memory = ChunkedMemoryService(vector_store, embedding_provider)
        
        # Check checkpoint
        checkpoint = message_storage.get_checkpoint(str(ctx.channel.id))
        
        # Fetch messages
        status_msg = await ctx.send("ðŸ”„ Fetching messages...")
        
        # Progress callback
        async def on_progress(progress):
            try:
                await status_msg.edit(
                    content=f"ðŸ”„ Fetching... {progress['processed']} messages "
                           f"({progress['rate']:.1f} msg/s) | "
                           f"Stored: {progress.get('successful', 0)}"
                )
            except Exception as e:
                # Ignore edit errors (message might be deleted)
                pass
        
        message_loader.set_progress_callback(on_progress)
        
        # Determine fetch strategy
        after_message = None
        if checkpoint:
            try:
                after_message = await ctx.channel.fetch_message(checkpoint['last_message_id'])
                fetch_type = "incremental"
            except Exception:
                # Message doesn't exist, do full fetch
                after_message = None
                fetch_type = "full"
        else:
            fetch_type = "full"
        
        # Load messages from Discord
        stats = await message_loader.load_channel_messages(
            ctx.channel,
            limit=limit,
            after=after_message,
            rate_limit_delay=rate_limit_delay
        )
        
        # Format and save messages
        await status_msg.edit(content="ðŸ’¾ Saving messages to database...")
        messages = []
        async for message in ctx.channel.history(
            limit=limit, 
            after=after_message,
            oldest_first=False
        ):
            if not message.author.bot and message.content.strip():
                if not message.content.startswith(Config.BOT_PREFIX):
                    messages.append(format_discord_message(message))
        
        if messages:
            message_storage.save_channel_messages(str(ctx.channel.id), messages)
        
        # Load all messages for chunking (from database)
        await status_msg.edit(content="ðŸ“¦ Loading messages from database...")
        all_messages = message_storage.load_channel_messages(str(ctx.channel.id))
        
        if not all_messages:
            await status_msg.edit(content="âŒ No messages found to chunk.")
            return
        
        # Chunk messages
        await status_msg.edit(content="ðŸ“¦ Chunking messages...")
        chunks_dict = chunking_service.chunk_messages(all_messages)
        
        # Store chunks
        await status_msg.edit(content="ðŸ’¾ Storing chunks in vector DB...")
        chunked_memory.store_all_strategies(chunks_dict)
        
        # Show results
        embed = discord.Embed(
            title="âœ… Chunking Complete",
            description=f"Processed {len(all_messages)} messages",
            color=discord.Color.green()
        )
        
        embed.add_field(
            name="Fetch Stats",
            value=(
                f"**Type:** {fetch_type}\n"
                f"**Processed:** {stats['total_processed']}\n"
                f"**Stored:** {stats['successfully_loaded']}\n"
                f"**Skipped:** {stats['skipped_bot_messages'] + stats['skipped_empty_messages']}"
            ),
            inline=False
        )
        
        stats_dict = chunked_memory.get_strategy_stats()
        chunk_info = "\n".join([f"**{s.title()}:** {c}" for s, c in stats_dict.items()])
        embed.add_field(
            name="Chunk Stats",
            value=chunk_info,
            inline=False
        )
        
        await status_msg.edit(content="", embed=embed)
        
    except Exception as e:
        self.logger.error(f"Error in chunk_channel: {e}", exc_info=True)
        await ctx.send(f"âŒ Error: {e}")

@commands.command(name='chunk_stats')
@commands.is_owner()
async def chunk_stats(self, ctx, channel_id: str = None):
    """Show chunking statistics"""
    if str(ctx.author.id) != str(Config.BOT_OWNER_ID):
        await ctx.send("ðŸš« **Access Denied!**")
        return
    
    try:
        vector_store = VectorStoreFactory.create()
        embedding_provider = EmbeddingServiceFactory.create()
        chunked_memory = ChunkedMemoryService(vector_store, embedding_provider)
        
        stats = chunked_memory.get_strategy_stats()
        
        embed = discord.Embed(
            title="ðŸ“Š Chunking Statistics",
            color=discord.Color.blue()
        )
        
        for strategy, count in stats.items():
            embed.add_field(name=strategy.title(), value=str(count), inline=True)
        
        await ctx.send(embed=embed)
    except Exception as e:
        await ctx.send(f"âŒ Error: {e}")

@commands.command(name='chunk_checkpoint')
@commands.is_owner()
async def chunk_checkpoint(self, ctx, channel_id: str = None):
    """Show checkpoint information"""
    if str(ctx.author.id) != str(Config.BOT_OWNER_ID):
        await ctx.send("ðŸš« **Access Denied!**")
        return
    
    try:
        channel_id = channel_id or str(ctx.channel.id)
        message_storage = MessageStorage()
        checkpoint = message_storage.get_checkpoint(channel_id)
        
        if checkpoint:
            embed = discord.Embed(
                title="ðŸ“ Checkpoint Information",
                color=discord.Color.blue()
            )
            embed.add_field(name="Channel ID", value=channel_id, inline=False)
            embed.add_field(name="Last Message ID", value=checkpoint['last_message_id'], inline=False)
            embed.add_field(name="Last Fetch", value=checkpoint['last_fetch_timestamp'], inline=False)
            embed.add_field(name="Total Messages", value=str(checkpoint['total_messages']), inline=True)
            await ctx.send(embed=embed)
        else:
            await ctx.send("âŒ No checkpoint found for this channel.")
    except Exception as e:
        await ctx.send(f"âŒ Error: {e}")
```

### Common Pitfalls - Phase 7

1. **Owner check**: Always verify owner before expensive operations
2. **Error handling**: Wrap everything in try/except
3. **Progress updates**: Handle edit errors gracefully
4. **Message limits**: Discord has message length limits
5. **Async/await**: All Discord operations must be awaited

---

## Phase 8: Summary Enhancement

### Learning Objectives
- Understand caching patterns
- Learn performance optimization
- Practice graceful degradation

### Implementation Steps

#### Step 8.1: Enhance Summary Command

Update `bot/cogs/summary.py`:

```python
from storage import MessageStorage
from config import Config

async def _fetch_messages_with_fallback(self, ctx, count: int) -> List[dict]:
    """
    Fetch messages using DB-first approach with Discord API fallback.
    
    Learning: Cache-first pattern improves performance.
    """
    message_storage = MessageStorage()
    checkpoint = message_storage.get_checkpoint(str(ctx.channel.id))
    
    if checkpoint and Config.SUMMARY_USE_STORED_MESSAGES:
        # Get from storage
        stored_messages = message_storage.get_recent_messages(
            str(ctx.channel.id),
            limit=count
        )
        
        # Check for gaps (new messages on Discord)
        try:
            # Try to fetch the latest message from Discord
            latest_discord_msg = await ctx.channel.fetch_message(
                checkpoint['last_message_id']
            )
            # If successful and we have enough stored messages, use them
            if stored_messages and len(stored_messages) >= count:
                return stored_messages[:count]
        except Exception:
            # Message doesn't exist or we're missing new ones
            pass
        
        # Fetch gap if needed
        if stored_messages:
            try:
                after_msg = await ctx.channel.fetch_message(
                    checkpoint['last_message_id']
                )
                new_messages = []
                async for msg in ctx.channel.history(
                    limit=count,
                    after=after_msg
                ):
                    if not msg.author.bot and msg.content.strip():
                        if not msg.content.startswith(ctx.prefix):
                            new_messages.append(format_discord_message(msg))
                
                # Merge and return
                all_messages = stored_messages + new_messages
                return all_messages[:count]
            except Exception as e:
                self.logger.warning(f"Error fetching gap: {e}, using stored only")
                return stored_messages[:count]
    
    # Fallback to Discord API
    return await self._fetch_messages(ctx, count)

# Update the summary command to use the new method
@commands.command(name="summary", help="Generate a summary of previous messages")
async def summary(self, ctx, count: int = 50):
    status_msg = await ctx.send("ðŸ” Fetching messages...")
    
    # Use DB-first approach
    messages = await self._fetch_messages_with_fallback(ctx, count)
    
    if not messages:
        await status_msg.edit(content="âŒ No messages found to summarize.")
        return
    
    # Rest of summary logic...
    await status_msg.edit(content=f"ðŸ“Š Analyzing {len(messages)} messages...")
    # ... existing summary code ...
```

---

## Phase 9: Configuration & Polish

### Learning Objectives
- Learn configuration management
- Practice feature flags
- Understand validation patterns

### Implementation Steps

#### Step 9.1: Add All Config Options

Update `config.py`:

```python
# Embedding Configuration
EMBEDDING_PROVIDER: str = os.getenv("EMBEDDING_PROVIDER", "sentence-transformers")

# Vector Store Configuration
VECTOR_STORE_PROVIDER: str = os.getenv("VECTOR_STORE_PROVIDER", "chroma")
PINECONE_API_KEY: Optional[str] = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT: Optional[str] = os.getenv("PINECONE_ENVIRONMENT")

# Chunking Configuration
CHUNKING_TEMPORAL_WINDOW: int = int(os.getenv("CHUNKING_TEMPORAL_WINDOW", "300"))
CHUNKING_CONVERSATION_GAP: int = int(os.getenv("CHUNKING_CONVERSATION_GAP", "1800"))

# Rate Limiting
MESSAGE_FETCH_DELAY: float = float(os.getenv("MESSAGE_FETCH_DELAY", "1.0"))
MESSAGE_FETCH_BATCH_SIZE: int = int(os.getenv("MESSAGE_FETCH_BATCH_SIZE", "100"))
MESSAGE_FETCH_PROGRESS_INTERVAL: int = int(os.getenv("MESSAGE_FETCH_PROGRESS_INTERVAL", "100"))
MESSAGE_FETCH_MAX_RETRIES: int = int(os.getenv("MESSAGE_FETCH_MAX_RETRIES", "5"))

# Storage Configuration
RAW_MESSAGES_DIR: str = os.getenv("RAW_MESSAGES_DIR", "data/raw_messages")

# Features
SUMMARY_USE_STORED_MESSAGES: bool = os.getenv(
    "SUMMARY_USE_STORED_MESSAGES", "True"
).lower() == "true"

@classmethod
def validate(cls) -> bool:
    """Validate configuration"""
    required = ["DISCORD_TOKEN"]
    missing = [var for var in required if not getattr(cls, var)]
    
    if missing:
        print(f"âŒ Missing required config: {', '.join(missing)}")
        return False
    
    # Validate rate limits
    if cls.MESSAGE_FETCH_DELAY < 0.1:
        print("âš ï¸ Warning: Rate limit delay too low, may get rate limited")
    
    # Validate chunking
    if cls.CHUNKING_TEMPORAL_WINDOW < 60:
        print("âš ï¸ Warning: Temporal window too small (< 60s)")
    
    # Validate embedding provider
    if cls.EMBEDDING_PROVIDER == "openai" and not cls.OPENAI_API_KEY:
        print("âš ï¸ Warning: OpenAI embedding provider selected but no API key")
    
    return True
```

---

## Quick Reference Checklists

### Phase 1 Checklist
- [ ] Database schema created
- [ ] MessageStorage class implemented
- [ ] Context managers for connections
- [ ] Checkpoint system working
- [ ] Tested with 100 messages
- [ ] Verified indexes work

### Phase 2 Checklist
- [ ] Rate limiting configured
- [ ] Exponential backoff implemented
- [ ] Progress callbacks working
- [ ] Error handling complete
- [ ] Tested with 1000 messages
- [ ] No rate limit errors

### Phase 3 Checklist
- [ ] Abstract base class created
- [ ] SentenceTransformer implemented
- [ ] OpenAI provider implemented
- [ ] Factory pattern working
- [ ] Both providers tested
- [ ] Dimensions verified

### Phase 4 Checklist
- [ ] Chunk data structure designed
- [ ] Temporal chunking implemented
- [ ] Conversation chunking implemented
- [ ] Single-message chunking implemented
- [ ] All strategies tested
- [ ] Metadata preserved

### Phase 5 Checklist
- [ ] Vector store abstract class created
- [ ] ChromaDB adapter implemented
- [ ] Factory pattern working
- [ ] Collections created correctly
- [ ] Queries working
- [ ] Error handling complete

### Phase 6 Checklist
- [ ] ChunkedMemoryService created
- [ ] Store all strategies working
- [ ] Search functionality working
- [ ] Stats collection working
- [ ] Strategy switching works
- [ ] End-to-end tested

### Phase 7 Checklist
- [ ] Bot commands integrated
- [ ] Progress updates working
- [ ] Error messages user-friendly
- [ ] All commands tested
- [ ] Owner checks in place
- [ ] Documentation complete

### Phase 8 Checklist
- [ ] Summary uses stored messages
- [ ] Gap detection working
- [ ] Fallback to API works
- [ ] Performance improved
- [ ] No regressions

### Phase 9 Checklist
- [ ] All config options added
- [ ] Validation working
- [ ] Feature flags working
- [ ] Environment variables documented
- [ ] Defaults are sensible

---

## Common Pitfalls & Debugging

### Database Issues

**Problem**: "database is locked"
- **Solution**: Use context managers, ensure connections are closed
- **Debug**: Check for long-running transactions

**Problem**: Slow queries
- **Solution**: Verify indexes are created, use EXPLAIN QUERY PLAN
- **Debug**: Check if queries are using indexes

### Rate Limiting Issues

**Problem**: Getting 429 errors
- **Solution**: Increase MESSAGE_FETCH_DELAY to 1.5-2.0 seconds
- **Debug**: Log actual request rate

**Problem**: Fetching too slow
- **Solution**: Balance between speed and safety (1.0s is good)
- **Debug**: Monitor progress callbacks

### Embedding Issues

**Problem**: Dimension mismatch
- **Solution**: Verify provider dimensions match collection
- **Debug**: Log embedding dimensions

**Problem**: API errors
- **Solution**: Add retry logic, check API keys
- **Debug**: Test with single encode first

### Chunking Issues

**Problem**: Empty chunks
- **Solution**: Check timestamp parsing, filter empty messages
- **Debug**: Print chunk contents

**Problem**: Too many/few chunks
- **Solution**: Adjust window sizes and gaps
- **Debug**: Visualize chunk boundaries

### Vector Store Issues

**Problem**: Collection not found
- **Solution**: Handle missing collections gracefully
- **Debug**: List all collections

**Problem**: Query returns nothing
- **Solution**: Verify documents were stored, check query format
- **Debug**: Query with known documents

---

## Testing Strategy

### For Each Phase:
1. **Unit Tests**: Test individual functions
2. **Integration Tests**: Test components together
3. **Performance Tests**: Measure with real data sizes
4. **Comparison Tests**: Compare strategies side-by-side

### Test Data Sizes:
- Phase 1-2: 100 messages (quick testing)
- Phase 3-4: 1,000 messages (medium testing)
- Phase 5-6: 10,000 messages (stress testing)

---

## Key Design Patterns Summary

1. **Strategy Pattern**: Embedding providers, chunking strategies
2. **Adapter Pattern**: Vector store wrappers
3. **Factory Pattern**: Creating providers from config
4. **Observer Pattern**: Progress callbacks
5. **Repository Pattern**: Message storage abstraction
6. **Dependency Injection**: Pass dependencies to services

---

## Next Steps

Start with **Phase 1** and work through sequentially. Each phase builds on the previous one. After completing each phase:

1. Test thoroughly
2. Understand the design decisions
3. Experiment with parameters
4. Document what you learned
5. Move to next phase

Good luck with your learning journey! ðŸš€
